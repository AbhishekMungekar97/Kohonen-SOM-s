{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a Kohonen Self Organizing Map for 24 randomly chosen colors:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "#creating a list of training the RGB color codes in decimal format \n",
    "Color_codes_decimal = [(220,20,60),(255,0,0),(255,99,71),(255,127,80),(205,92,92),(240,128,128),(233,150,122),(250,128,114),(255,255,0),(154,205,50),(85,107,47),(107,142,35),(124,252,0),(127,255,0),(173,255,47),(0,100,0),(25,25,112),(0,0,128),(0,0,139),(0,0,205),(0,0,255),(65,105,225),(138,43,226),(75,0,130)]\n",
    "\n",
    "#To simplify the task at hand, we convert the decimal RGB values within the range of 0 to 1 \n",
    "Color_codes_0_to_1 = []\n",
    "for x,y,z in Color_codes_decimal :\n",
    "    x = x/255\n",
    "    y = y/255\n",
    "    z= z/255\n",
    "    k = (x,y,z)\n",
    "    Color_codes_0_to_1.append(k)\n",
    "    \n",
    "Color_codes_0_to_1 = np.array(Color_codes_0_to_1)\n",
    "Color_codes_0_to_1 = Color_codes_0_to_1.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following few cases, we will be looking at 5 different cases, where the ability of the self organizing maps will be observed over varying ranges of \"Initialized sigma\" and \"Epochs\"\n",
    "### - Case 1 : Analyzing the Self organizing map for initial sigma = 1 over a range of 20,40,100 and 1000 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing the necessary parameters \n",
    "data = Color_codes_0_to_1\n",
    "\n",
    "# Declaring the grid of neurons\n",
    "neuron_grid = np.array([100, 100])\n",
    "\n",
    "#Initial learning rate\n",
    "initial_learning_rate = 0.8\n",
    "\n",
    "num_of_features = data.shape[0]\n",
    "num_of_samples = data.shape[1]\n",
    "\n",
    "# initial sigma with regards to the neighbourhood function\n",
    "initial_sigma = 1\n",
    "\n",
    "# The number of epochs\n",
    "num_of_iterations = [20,40,100,1000]\n",
    "\n",
    "# Randomizing the weights : will be utilized throughout the model\n",
    "Random_weights = np.random.random((neuron_grid[0], neuron_grid[1], num_of_features))\n",
    "\n",
    "#Declaring neccesary functions :-\n",
    "#Declaring the sigma function, which decays over time\n",
    "def Sigma(initial_radius, i, num_of_iterations ):\n",
    "    return initial_radius * np.exp(-i / num_of_iterations )\n",
    "\n",
    "# Declaring the decaying learning rate function\n",
    "def decay_learning_rate(initial_learning_rate, i, num_of_iterations):\n",
    "    return initial_learning_rate * np.exp(-i / num_of_iterations)\n",
    "\n",
    "# Developing the neighbourhood function\n",
    "def Neighbourhood_func(distance, radius):\n",
    "    return np.exp(-distance / (2* (radius**2)))\n",
    "\n",
    "# Tabulating the best neuron for the input vector\n",
    "def best_neuron(train_data, Random_weights, num_of_features):\n",
    "    \n",
    "    # finding the index of the neuron to which the input may belong to\n",
    "    best_neuron_index = np.array([0, 0])\n",
    "    min_dist = np.iinfo(np.int).max\n",
    "    \n",
    "    # Tabulating the distance between each neuron and the input\n",
    "    for x in range(Random_weights.shape[0]):\n",
    "        for y in range(Random_weights.shape[1]):\n",
    "            w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "            square_distance = np.sum((w - train_data) ** 2)\n",
    "            square_distance = np.sqrt(square_distance)\n",
    "            if square_distance < min_dist:\n",
    "                min_dist = square_distance \n",
    "                best_neuron_index = np.array([x, y]) \n",
    "    \n",
    "    best_neuron_unit = Random_weights[best_neuron_index[0], best_neuron_index[1], :].reshape(num_of_features, 1)\n",
    "    return (best_neuron_unit, best_neuron_index)\n",
    "\n",
    "# Training the model to find the winning neurons\n",
    "for k in num_of_iterations:\n",
    "    for i in range(k):\n",
    "        # obtaining a training data at random \n",
    "        train_data = data[:, np.random.randint(0, num_of_samples)].reshape(np.array([num_of_features, 1]))\n",
    "    \n",
    "        # Finding the best neuron\n",
    "        best_neuron_unit, best_neuron_index = best_neuron(train_data, Random_weights,num_of_features)\n",
    "        \n",
    "        # Tabulating the decaying learning rate and sigma\n",
    "        Decaying_sigma = Sigma(initial_sigma, i,k)\n",
    "        Decaying_learning_rate = decay_learning_rate(initial_learning_rate, i, k)\n",
    "    \n",
    "        # Updating the weights of the winning neuron\n",
    "        for x in range(Random_weights.shape[0]):\n",
    "            for y in range(Random_weights.shape[1]):\n",
    "                w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "                w_dist = np.sum((np.array([x, y]) - best_neuron_index) ** 2)\n",
    "                w_dist = np.sqrt(w_dist)\n",
    "            \n",
    "                if w_dist <= Decaying_sigma:\n",
    "                    # calculating the topological neighbourhood\n",
    "                    N = Neighbourhood_func(w_dist, Decaying_sigma)\n",
    "                    new_w = w + (Decaying_learning_rate * N * (train_data - w))\n",
    "                    Random_weights[x, y, :] = new_w.reshape(1, 3)\n",
    "        #Plotting the neuron grid to analyze the influence\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.set_xlim((0, Random_weights.shape[0]+1))\n",
    "    ax.set_ylim((0, Random_weights.shape[1]+1))\n",
    "    ax.set_title('Kohnens Self-organizing map for an epoch '+ str(i+1) + ' with a sigma of 1 ')\n",
    "    for x in range(1, Random_weights.shape[0] + 1):\n",
    "        for y in range(1, Random_weights.shape[1] + 1):\n",
    "            ax.add_patch(patches.Circle((x, y), 0.5,facecolor=Random_weights[x-1,y-1,:],edgecolor='none'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Case 2 : Analyzing the Self organizing map for initial sigma = 10 over a range of 20,40,100 and 1000 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing the necessary parameters \n",
    "data = Color_codes_0_to_1\n",
    "\n",
    "# Declaring the grid of neurons\n",
    "neuron_grid = np.array([100, 100])\n",
    "\n",
    "#Initial learning rate\n",
    "initial_learning_rate = 0.8\n",
    "\n",
    "num_of_features = data.shape[0]\n",
    "num_of_samples = data.shape[1]\n",
    "\n",
    "# initial sigma with regards to the neighbourhood function\n",
    "initial_sigma = 10\n",
    "\n",
    "# The number of epochs\n",
    "num_of_iterations = [20,40,100,1000]\n",
    "\n",
    "# Randomizing the weights : will be utilized throughout the model\n",
    "Random_weights = np.random.random((neuron_grid[0], neuron_grid[1], num_of_features))\n",
    "\n",
    "#Declaring neccesary functions :-\n",
    "#Declaring the sigma function, which decays over time\n",
    "def Sigma(initial_radius, i, num_of_iterations ):\n",
    "    return initial_radius * np.exp(-i / num_of_iterations )\n",
    "\n",
    "# Declaring the decaying learning rate function\n",
    "def decay_learning_rate(initial_learning_rate, i, num_of_iterations):\n",
    "    return initial_learning_rate * np.exp(-i / num_of_iterations)\n",
    "\n",
    "# Developing the neighbourhood function\n",
    "def Neighbourhood_func(distance, radius):\n",
    "    return np.exp(-distance / (2* (radius**2)))\n",
    "\n",
    "# Tabulating the best neuron for the input vector\n",
    "def best_neuron(train_data, Random_weights, num_of_features):\n",
    "    \n",
    "    # finding the index of the neuron to which the input may belong to\n",
    "    best_neuron_index = np.array([0, 0])\n",
    "    min_dist = np.iinfo(np.int).max\n",
    "    \n",
    "    # Tabulating the distance between each neuron and the input\n",
    "    for x in range(Random_weights.shape[0]):\n",
    "        for y in range(Random_weights.shape[1]):\n",
    "            w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "            square_distance = np.sum((w - train_data) ** 2)\n",
    "            square_distance = np.sqrt(square_distance)\n",
    "            if square_distance < min_dist:\n",
    "                min_dist = square_distance \n",
    "                best_neuron_index = np.array([x, y]) \n",
    "    \n",
    "    best_neuron_unit = Random_weights[best_neuron_index[0], best_neuron_index[1], :].reshape(num_of_features, 1)\n",
    "    return (best_neuron_unit, best_neuron_index)\n",
    "\n",
    "# Training the model to find the winning neurons\n",
    "for k in num_of_iterations:\n",
    "    for i in range(k):\n",
    "        # obtaining a training data at random \n",
    "        train_data = data[:, np.random.randint(0, num_of_samples)].reshape(np.array([num_of_features, 1]))\n",
    "    \n",
    "        # Finding the best neuron\n",
    "        best_neuron_unit, best_neuron_index = best_neuron(train_data, Random_weights,num_of_features)\n",
    "        \n",
    "        # Tabulating the decaying learning rate and sigma\n",
    "        Decaying_sigma = Sigma(initial_sigma, i,k)\n",
    "        Decaying_learning_rate = decay_learning_rate(initial_learning_rate, i, k)\n",
    "    \n",
    "        # Updating the weights of the winning neuron\n",
    "        for x in range(Random_weights.shape[0]):\n",
    "            for y in range(Random_weights.shape[1]):\n",
    "                w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "                w_dist = np.sum((np.array([x, y]) - best_neuron_index) ** 2)\n",
    "                w_dist = np.sqrt(w_dist)\n",
    "            \n",
    "                if w_dist <= Decaying_sigma:\n",
    "                    # calculating the topological neighbourhood\n",
    "                    N = Neighbourhood_func(w_dist, Decaying_sigma)\n",
    "                    new_w = w + (Decaying_learning_rate * N * (train_data - w))\n",
    "                    Random_weights[x, y, :] = new_w.reshape(1, 3)\n",
    "        #Plotting the neuron grid to analyze the influence\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.set_xlim((0, Random_weights.shape[0]+1))\n",
    "    ax.set_ylim((0, Random_weights.shape[1]+1))\n",
    "    ax.set_title('Kohnens Self-organizing map for an epoch '+ str(i+1) + ' with a sigma of 10 ')\n",
    "    for x in range(1, Random_weights.shape[0] + 1):\n",
    "        for y in range(1, Random_weights.shape[1] + 1):\n",
    "            ax.add_patch(patches.Circle((x, y), 0.5,facecolor=Random_weights[x-1,y-1,:],edgecolor='none'))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Case 3 : Analyzing the Self organizing map for initial sigma = 30 over a range of 20,40,100 and 1000 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing the necessary parameters \n",
    "data = Color_codes_0_to_1\n",
    "\n",
    "# Declaring the grid of neurons\n",
    "neuron_grid = np.array([100, 100])\n",
    "\n",
    "#Initial learning rate\n",
    "initial_learning_rate = 0.8\n",
    "\n",
    "num_of_features = data.shape[0]\n",
    "num_of_samples = data.shape[1]\n",
    "\n",
    "# initial sigma with regards to the neighbourhood function\n",
    "initial_sigma = 30\n",
    "\n",
    "# The number of epochs\n",
    "num_of_iterations = [20,40,100,1000]\n",
    "\n",
    "# Randomizing the weights : will be utilized throughout the model\n",
    "Random_weights = np.random.random((neuron_grid[0], neuron_grid[1], num_of_features))\n",
    "\n",
    "#Declaring neccesary functions :-\n",
    "#Declaring the sigma function, which decays over time\n",
    "def Sigma(initial_radius, i, num_of_iterations ):\n",
    "    return initial_radius * np.exp(-i / num_of_iterations )\n",
    "\n",
    "# Declaring the decaying learning rate function\n",
    "def decay_learning_rate(initial_learning_rate, i, num_of_iterations):\n",
    "    return initial_learning_rate * np.exp(-i / num_of_iterations)\n",
    "\n",
    "# Developing the neighbourhood function\n",
    "def Neighbourhood_func(distance, radius):\n",
    "    return np.exp(-distance / (2* (radius**2)))\n",
    "\n",
    "# Tabulating the best neuron for the input vector\n",
    "def best_neuron(train_data, Random_weights, num_of_features):\n",
    "    \n",
    "    # finding the index of the neuron to which the input may belong to\n",
    "    best_neuron_index = np.array([0, 0])\n",
    "    min_dist = np.iinfo(np.int).max\n",
    "    \n",
    "    # Tabulating the distance between each neuron and the input\n",
    "    for x in range(Random_weights.shape[0]):\n",
    "        for y in range(Random_weights.shape[1]):\n",
    "            w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "            square_distance = np.sum((w - train_data) ** 2)\n",
    "            square_distance = np.sqrt(square_distance)\n",
    "            if square_distance < min_dist:\n",
    "                min_dist = square_distance \n",
    "                best_neuron_index = np.array([x, y]) \n",
    "    \n",
    "    best_neuron_unit = Random_weights[best_neuron_index[0], best_neuron_index[1], :].reshape(num_of_features, 1)\n",
    "    return (best_neuron_unit, best_neuron_index)\n",
    "\n",
    "# Training the model to find the winning neurons\n",
    "for k in num_of_iterations:\n",
    "    for i in range(k):\n",
    "        # obtaining a training data at random \n",
    "        train_data = data[:, np.random.randint(0, num_of_samples)].reshape(np.array([num_of_features, 1]))\n",
    "    \n",
    "        # Finding the best neuron\n",
    "        best_neuron_unit, best_neuron_index = best_neuron(train_data, Random_weights,num_of_features)\n",
    "        \n",
    "        # Tabulating the decaying learning rate and sigma\n",
    "        Decaying_sigma = Sigma(initial_sigma, i,k)\n",
    "        Decaying_learning_rate = decay_learning_rate(initial_learning_rate, i, k)\n",
    "    \n",
    "        # Updating the weights of the winning neuron\n",
    "        for x in range(Random_weights.shape[0]):\n",
    "            for y in range(Random_weights.shape[1]):\n",
    "                w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "                w_dist = np.sum((np.array([x, y]) - best_neuron_index) ** 2)\n",
    "                w_dist = np.sqrt(w_dist)\n",
    "            \n",
    "                if w_dist <= Decaying_sigma:\n",
    "                    # calculating the topological neighbourhood\n",
    "                    N = Neighbourhood_func(w_dist, Decaying_sigma)\n",
    "                    new_w = w + (Decaying_learning_rate * N * (train_data - w))\n",
    "                    Random_weights[x, y, :] = new_w.reshape(1, 3)\n",
    "        #Plotting the neuron grid to analyze the influence\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.set_xlim((0, Random_weights.shape[0]+1))\n",
    "    ax.set_ylim((0, Random_weights.shape[1]+1))\n",
    "    ax.set_title('Kohnens Self-organizing map for an epoch '+ str(i+1) + ' with a sigma of 30 ')\n",
    "    for x in range(1, Random_weights.shape[0] + 1):\n",
    "        for y in range(1, Random_weights.shape[1] + 1):\n",
    "            ax.add_patch(patches.Circle((x, y), 0.5,facecolor=Random_weights[x-1,y-1,:],edgecolor='none'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Case 4 : Analyzing the Self organizing map for initial sigma = 50 over a range of 20,40,100 and 1000 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing the necessary parameters \n",
    "data = Color_codes_0_to_1\n",
    "\n",
    "# Declaring the grid of neurons\n",
    "neuron_grid = np.array([100, 100])\n",
    "\n",
    "#Initial learning rate\n",
    "initial_learning_rate = 0.8\n",
    "\n",
    "num_of_features = data.shape[0]\n",
    "num_of_samples = data.shape[1]\n",
    "\n",
    "# initial sigma with regards to the neighbourhood function\n",
    "initial_sigma = 50\n",
    "\n",
    "# The number of epochs\n",
    "num_of_iterations = [20,40,100,1000]\n",
    "\n",
    "# Randomizing the weights : will be utilized throughout the model\n",
    "Random_weights = np.random.random((neuron_grid[0], neuron_grid[1], num_of_features))\n",
    "\n",
    "#Declaring neccesary functions :-\n",
    "#Declaring the sigma function, which decays over time\n",
    "def Sigma(initial_radius, i, num_of_iterations ):\n",
    "    return initial_radius * np.exp(-i / num_of_iterations )\n",
    "\n",
    "# Declaring the decaying learning rate function\n",
    "def decay_learning_rate(initial_learning_rate, i, num_of_iterations):\n",
    "    return initial_learning_rate * np.exp(-i / num_of_iterations)\n",
    "\n",
    "# Developing the neighbourhood function\n",
    "def Neighbourhood_func(distance, radius):\n",
    "    return np.exp(-distance / (2* (radius**2)))\n",
    "\n",
    "# Tabulating the best neuron for the input vector\n",
    "def best_neuron(train_data, Random_weights, num_of_features):\n",
    "    \n",
    "    # finding the index of the neuron to which the input may belong to\n",
    "    best_neuron_index = np.array([0, 0])\n",
    "    min_dist = np.iinfo(np.int).max\n",
    "    \n",
    "    # Tabulating the distance between each neuron and the input\n",
    "    for x in range(Random_weights.shape[0]):\n",
    "        for y in range(Random_weights.shape[1]):\n",
    "            w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "            square_distance = np.sum((w - train_data) ** 2)\n",
    "            square_distance = np.sqrt(square_distance)\n",
    "            if square_distance < min_dist:\n",
    "                min_dist = square_distance \n",
    "                best_neuron_index = np.array([x, y]) \n",
    "    \n",
    "    best_neuron_unit = Random_weights[best_neuron_index[0], best_neuron_index[1], :].reshape(num_of_features, 1)\n",
    "    return (best_neuron_unit, best_neuron_index)\n",
    "\n",
    "# Training the model to find the winning neurons\n",
    "for k in num_of_iterations:\n",
    "    for i in range(k):\n",
    "        # obtaining a training data at random \n",
    "        train_data = data[:, np.random.randint(0, num_of_samples)].reshape(np.array([num_of_features, 1]))\n",
    "    \n",
    "        # Finding the best neuron\n",
    "        best_neuron_unit, best_neuron_index = best_neuron(train_data, Random_weights,num_of_features)\n",
    "        \n",
    "        # Tabulating the decaying learning rate and sigma\n",
    "        Decaying_sigma = Sigma(initial_sigma, i,k)\n",
    "        Decaying_learning_rate = decay_learning_rate(initial_learning_rate, i, k)\n",
    "    \n",
    "        # Updating the weights of the winning neuron\n",
    "        for x in range(Random_weights.shape[0]):\n",
    "            for y in range(Random_weights.shape[1]):\n",
    "                w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "                w_dist = np.sum((np.array([x, y]) - best_neuron_index) ** 2)\n",
    "                w_dist = np.sqrt(w_dist)\n",
    "            \n",
    "                if w_dist <= Decaying_sigma:\n",
    "                    # calculating the topological neighbourhood\n",
    "                    N = Neighbourhood_func(w_dist, Decaying_sigma)\n",
    "                    new_w = w + (Decaying_learning_rate * N * (train_data - w))\n",
    "                    Random_weights[x, y, :] = new_w.reshape(1, 3)\n",
    "        #Plotting the neuron grid to analyze the influence\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.set_xlim((0, Random_weights.shape[0]+1))\n",
    "    ax.set_ylim((0, Random_weights.shape[1]+1))\n",
    "    ax.set_title('Kohnens Self-organizing map for an epoch '+ str(i+1) + ' with a sigma of 50 ')\n",
    "    for x in range(1, Random_weights.shape[0] + 1):\n",
    "        for y in range(1, Random_weights.shape[1] + 1):\n",
    "            ax.add_patch(patches.Circle((x, y), 0.5,facecolor=Random_weights[x-1,y-1,:],edgecolor='none'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Case 5 : Analyzing the Self organizing map for initial sigma = 70 over a range of 20,40,100 and 1000 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing the necessary parameters \n",
    "data = Color_codes_0_to_1\n",
    "\n",
    "# Declaring the grid of neurons\n",
    "neuron_grid = np.array([100, 100])\n",
    "\n",
    "#Initial learning rate\n",
    "initial_learning_rate = 0.8\n",
    "\n",
    "num_of_features = data.shape[0]\n",
    "num_of_samples = data.shape[1]\n",
    "\n",
    "# initial sigma with regards to the neighbourhood function\n",
    "initial_sigma = 70\n",
    "\n",
    "# The number of epochs\n",
    "num_of_iterations = [20,40,100,1000]\n",
    "\n",
    "# Randomizing the weights : will be utilized throughout the model\n",
    "Random_weights = np.random.random((neuron_grid[0], neuron_grid[1], num_of_features))\n",
    "\n",
    "#Declaring neccesary functions :-\n",
    "#Declaring the sigma function, which decays over time\n",
    "def Sigma(initial_radius, i, num_of_iterations ):\n",
    "    return initial_radius * np.exp(-i / num_of_iterations )\n",
    "\n",
    "# Declaring the decaying learning rate function\n",
    "def decay_learning_rate(initial_learning_rate, i, num_of_iterations):\n",
    "    return initial_learning_rate * np.exp(-i / num_of_iterations)\n",
    "\n",
    "# Developing the neighbourhood function\n",
    "def Neighbourhood_func(distance, radius):\n",
    "    return np.exp(-distance / (2* (radius**2)))\n",
    "\n",
    "# Tabulating the best neuron for the input vector\n",
    "def best_neuron(train_data, Random_weights, num_of_features):\n",
    "    \n",
    "    # finding the index of the neuron to which the input may belong to\n",
    "    best_neuron_index = np.array([0, 0])\n",
    "    min_dist = np.iinfo(np.int).max\n",
    "    \n",
    "    # Tabulating the distance between each neuron and the input\n",
    "    for x in range(Random_weights.shape[0]):\n",
    "        for y in range(Random_weights.shape[1]):\n",
    "            w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "            square_distance = np.sum((w - train_data) ** 2)\n",
    "            square_distance = np.sqrt(square_distance)\n",
    "            if square_distance < min_dist:\n",
    "                min_dist = square_distance \n",
    "                best_neuron_index = np.array([x, y]) \n",
    "    \n",
    "    best_neuron_unit = Random_weights[best_neuron_index[0], best_neuron_index[1], :].reshape(num_of_features, 1)\n",
    "    return (best_neuron_unit, best_neuron_index)\n",
    "\n",
    "# Training the model to find the winning neurons\n",
    "for k in num_of_iterations:\n",
    "    for i in range(k):\n",
    "        # obtaining a training data at random \n",
    "        train_data = data[:, np.random.randint(0, num_of_samples)].reshape(np.array([num_of_features, 1]))\n",
    "    \n",
    "        # Finding the best neuron\n",
    "        best_neuron_unit, best_neuron_index = best_neuron(train_data, Random_weights,num_of_features)\n",
    "        \n",
    "        # Tabulating the decaying learning rate and sigma\n",
    "        Decaying_sigma = Sigma(initial_sigma, i,k)\n",
    "        Decaying_learning_rate = decay_learning_rate(initial_learning_rate, i, k)\n",
    "    \n",
    "        # Updating the weights of the winning neuron\n",
    "        for x in range(Random_weights.shape[0]):\n",
    "            for y in range(Random_weights.shape[1]):\n",
    "                w = Random_weights[x, y, :].reshape(num_of_features, 1)\n",
    "                w_dist = np.sum((np.array([x, y]) - best_neuron_index) ** 2)\n",
    "                w_dist = np.sqrt(w_dist)\n",
    "            \n",
    "                if w_dist <= Decaying_sigma:\n",
    "                    # calculating the topological neighbourhood\n",
    "                    N = Neighbourhood_func(w_dist, Decaying_sigma)\n",
    "                    new_w = w + (Decaying_learning_rate * N * (train_data - w))\n",
    "                    Random_weights[x, y, :] = new_w.reshape(1, 3)\n",
    "        #Plotting the neuron grid to analyze the influence\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.set_xlim((0, Random_weights.shape[0]+1))\n",
    "    ax.set_ylim((0, Random_weights.shape[1]+1))\n",
    "    ax.set_title('Kohnens Self-organizing map for an epoch '+ str(i+1) + ' with a sigma of 70 ')\n",
    "    for x in range(1, Random_weights.shape[0] + 1):\n",
    "        for y in range(1, Random_weights.shape[1] + 1):\n",
    "            ax.add_patch(patches.Circle((x, y), 0.5,facecolor=Random_weights[x-1,y-1,:],edgecolor='none'))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
